2025-04-09 14:21:59,882 INFO: Attempting to get available models... [in C:\AIapps\MIDAS3\app.py:49]
2025-04-09 14:21:59,886 INFO: Available models: ['llama3-groq-tool-use:8b', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest'] [in C:\AIapps\MIDAS3\app.py:53]
2025-04-09 14:21:59,887 INFO: Serialized models: [{'name': 'llama3-groq-tool-use:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}] [in C:\AIapps\MIDAS3\app.py:66]
2025-04-09 14:22:00,204 ERROR: Error listing conversations: name 'app' is not defined [in C:\AIapps\MIDAS3\app.py:92]
Traceback (most recent call last):
  File "C:\AIapps\MIDAS3\database.py", line 88, in get_all_conversations
    raise RuntimeError("Database connection not established")
RuntimeError: Database connection not established

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\AIapps\MIDAS3\app.py", line 85, in list_conversations
    conversations = db.get_all_conversations()
  File "C:\AIapps\MIDAS3\database.py", line 114, in get_all_conversations
    app.logger.error(f"Database error in get_all_conversations: {str(e)}", exc_info=True)
NameError: name 'app' is not defined
2025-04-09 14:24:23,550 INFO: Attempting to get available models... [in C:\AIapps\MIDAS3\app.py:49]
2025-04-09 14:24:23,551 INFO: Available models: ['llama3-groq-tool-use:8b', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest'] [in C:\AIapps\MIDAS3\app.py:53]
2025-04-09 14:24:23,551 INFO: Serialized models: [{'name': 'llama3-groq-tool-use:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}] [in C:\AIapps\MIDAS3\app.py:66]
2025-04-09 14:24:23,877 ERROR: Error listing conversations: Database connection not established. Call init_app() first. [in C:\AIapps\MIDAS3\app.py:92]
Traceback (most recent call last):
  File "C:\AIapps\MIDAS3\app.py", line 85, in list_conversations
    conversations = db.get_all_conversations()
  File "C:\AIapps\MIDAS3\database.py", line 94, in get_all_conversations
    raise RuntimeError("Database connection not established. Call init_app() first.")
RuntimeError: Database connection not established. Call init_app() first.
2025-04-09 14:30:19,264 INFO: Attempting to get available models... [in C:\AIapps\MIDAS3\app.py:65]
2025-04-09 14:30:19,265 INFO: Available models: ['llama3-groq-tool-use:8b', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest'] [in C:\AIapps\MIDAS3\app.py:69]
2025-04-09 14:30:19,265 INFO: Serialized models: [{'name': 'llama3-groq-tool-use:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}] [in C:\AIapps\MIDAS3\app.py:82]
2025-04-09 14:30:19,523 ERROR: Error listing conversations: (sqlite3.OperationalError) no such column: conversations.model
[SQL: SELECT conversations.id AS conversations_id, conversations.title AS conversations_title, conversations.created_at AS conversations_created_at, conversations.updated_at AS conversations_updated_at, conversations.model AS conversations_model 
FROM conversations ORDER BY conversations.updated_at DESC]
(Background on this error at: https://sqlalche.me/e/20/e3q8) [in C:\AIapps\MIDAS3\app.py:108]
Traceback (most recent call last):
  File "C:\Users\Cyber Beast Tech\.conda\envs\midas3-llm\lib\site-packages\sqlalchemy\engine\base.py", line 1964, in _exec_single_context
    self.dialect.do_execute(
  File "C:\Users\Cyber Beast Tech\.conda\envs\midas3-llm\lib\site-packages\sqlalchemy\engine\default.py", line 945, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: no such column: conversations.model

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\AIapps\MIDAS3\app.py", line 101, in list_conversations
    conversations = get_all_conversations()
  File "C:\AIapps\MIDAS3\database.py", line 60, in get_all_conversations
    conversations = Conversation.query.order_by(Conversation.updated_at.desc()).all()
  File "C:\Users\Cyber Beast Tech\.conda\envs\midas3-llm\lib\site-packages\sqlalchemy\orm\query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "C:\Users\Cyber Beast Tech\.conda\envs\midas3-llm\lib\site-packages\sqlalchemy\orm\query.py", line 2858, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "C:\Users\Cyber Beast Tech\.conda\envs\midas3-llm\lib\site-packages\sqlalchemy\orm\session.py", line 2365, in execute
    return self._execute_internal(
  File "C:\Users\Cyber Beast Tech\.conda\envs\midas3-llm\lib\site-packages\sqlalchemy\orm\session.py", line 2251, in _execute_internal
    result: Result[Any] = compile_state_cls.orm_execute_statement(
  File "C:\Users\Cyber Beast Tech\.conda\envs\midas3-llm\lib\site-packages\sqlalchemy\orm\context.py", line 306, in orm_execute_statement
    result = conn.execute(
  File "C:\Users\Cyber Beast Tech\.conda\envs\midas3-llm\lib\site-packages\sqlalchemy\engine\base.py", line 1416, in execute
    return meth(
  File "C:\Users\Cyber Beast Tech\.conda\envs\midas3-llm\lib\site-packages\sqlalchemy\sql\elements.py", line 523, in _execute_on_connection
    return connection._execute_clauseelement(
  File "C:\Users\Cyber Beast Tech\.conda\envs\midas3-llm\lib\site-packages\sqlalchemy\engine\base.py", line 1638, in _execute_clauseelement
    ret = self._execute_context(
  File "C:\Users\Cyber Beast Tech\.conda\envs\midas3-llm\lib\site-packages\sqlalchemy\engine\base.py", line 1843, in _execute_context
    return self._exec_single_context(
  File "C:\Users\Cyber Beast Tech\.conda\envs\midas3-llm\lib\site-packages\sqlalchemy\engine\base.py", line 1983, in _exec_single_context
    self._handle_dbapi_exception(
  File "C:\Users\Cyber Beast Tech\.conda\envs\midas3-llm\lib\site-packages\sqlalchemy\engine\base.py", line 2352, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "C:\Users\Cyber Beast Tech\.conda\envs\midas3-llm\lib\site-packages\sqlalchemy\engine\base.py", line 1964, in _exec_single_context
    self.dialect.do_execute(
  File "C:\Users\Cyber Beast Tech\.conda\envs\midas3-llm\lib\site-packages\sqlalchemy\engine\default.py", line 945, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such column: conversations.model
[SQL: SELECT conversations.id AS conversations_id, conversations.title AS conversations_title, conversations.created_at AS conversations_created_at, conversations.updated_at AS conversations_updated_at, conversations.model AS conversations_model 
FROM conversations ORDER BY conversations.updated_at DESC]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2025-04-09 14:33:14,041 INFO: Attempting to get available models... [in C:\AIapps\MIDAS3\app.py:65]
2025-04-09 14:33:14,041 INFO: Available models: ['llama3-groq-tool-use:8b', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest'] [in C:\AIapps\MIDAS3\app.py:69]
2025-04-09 14:33:14,042 INFO: Serialized models: [{'name': 'llama3-groq-tool-use:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}] [in C:\AIapps\MIDAS3\app.py:82]
2025-04-09 14:33:17,300 ERROR: Error creating conversation: create_conversation() takes 0 positional arguments but 1 was given [in C:\AIapps\MIDAS3\app.py:128]
2025-04-09 14:34:09,119 INFO: Attempting to get available models... [in C:\AIapps\MIDAS3\app.py:65]
2025-04-09 14:34:09,121 INFO: Available models: ['llama3-groq-tool-use:8b', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest'] [in C:\AIapps\MIDAS3\app.py:69]
2025-04-09 14:34:09,123 INFO: Serialized models: [{'name': 'llama3-groq-tool-use:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}] [in C:\AIapps\MIDAS3\app.py:82]
2025-04-09 14:34:12,275 ERROR: Error creating conversation: create_conversation() takes 0 positional arguments but 1 was given [in C:\AIapps\MIDAS3\app.py:128]
2025-04-09 14:35:00,275 INFO: Attempting to get available models... [in C:\AIapps\MIDAS3\app.py:65]
2025-04-09 14:35:00,276 INFO: Available models: ['llama3-groq-tool-use:8b', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest'] [in C:\AIapps\MIDAS3\app.py:69]
2025-04-09 14:35:00,276 INFO: Serialized models: [{'name': 'llama3-groq-tool-use:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}] [in C:\AIapps\MIDAS3\app.py:82]
2025-04-09 14:36:28,281 INFO: Attempting to get available models... [in C:\AIapps\MIDAS3\app.py:65]
2025-04-09 14:36:28,282 INFO: Available models: ['llama3-groq-tool-use:8b', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest'] [in C:\AIapps\MIDAS3\app.py:69]
2025-04-09 14:36:28,284 INFO: Serialized models: [{'name': 'llama3-groq-tool-use:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}] [in C:\AIapps\MIDAS3\app.py:82]
2025-04-09 14:37:51,587 INFO: Attempting to get available models... [in C:\AIapps\MIDAS3\app.py:67]
2025-04-09 14:37:51,588 INFO: Available models: ['llama3-groq-tool-use:8b', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest'] [in C:\AIapps\MIDAS3\app.py:71]
2025-04-09 14:37:51,588 INFO: Serialized models: [{'name': 'llama3-groq-tool-use:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}] [in C:\AIapps\MIDAS3\app.py:84]
2025-04-09 14:39:37,211 INFO: Attempting to get available models... [in C:\AIapps\MIDAS3\app.py:67]
2025-04-09 14:39:37,212 INFO: Available models: ['llama3-groq-tool-use:8b', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest'] [in C:\AIapps\MIDAS3\app.py:71]
2025-04-09 14:39:37,213 INFO: Serialized models: [{'name': 'llama3-groq-tool-use:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}] [in C:\AIapps\MIDAS3\app.py:84]
2025-04-09 14:39:59,082 INFO: Attempting to get available models... [in C:\AIapps\MIDAS3\app.py:67]
2025-04-09 14:39:59,084 INFO: Available models: ['llama3-groq-tool-use:8b', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest'] [in C:\AIapps\MIDAS3\app.py:71]
2025-04-09 14:39:59,084 INFO: Serialized models: [{'name': 'llama3-groq-tool-use:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}] [in C:\AIapps\MIDAS3\app.py:84]
2025-04-09 14:40:05,904 ERROR: Error deleting conversation: maximum recursion depth exceeded while calling a Python object [in C:\AIapps\MIDAS3\app.py:292]
2025-04-09 14:40:08,091 INFO: Attempting to get available models... [in C:\AIapps\MIDAS3\app.py:67]
2025-04-09 14:40:08,092 INFO: Available models: ['llama3-groq-tool-use:8b', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest'] [in C:\AIapps\MIDAS3\app.py:71]
2025-04-09 14:40:08,093 INFO: Serialized models: [{'name': 'llama3-groq-tool-use:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}] [in C:\AIapps\MIDAS3\app.py:84]
2025-06-06 16:55:08 - app - INFO - Logging configured successfully
2025-06-06 16:55:31 - app - INFO - Attempting to get available models...
2025-06-06 16:55:31 - app - INFO - User preferred models: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 16:55:31 - app - INFO - Available models from Ollama: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 16:55:31 - app - INFO - Filtered models based on user preferences: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 16:55:31 - app - INFO - Serialized models: [{'name': 'mistral-small3.1:latest', 'details': {'parameter_size': ''}}, {'name': 'mannix/llama3.1-8b-abliterated:latest', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}]
2025-06-06 16:55:40 - app - INFO - File 'aiss-technical-specs.pdf' saved successfully to 'C:\AIapps\MIDAS3\docs\conversation_4\aiss-technical-specs.pdf'
2025-06-06 16:55:50 - app - INFO - Quota check result: {'allowed': True, 'daily_limit': 100, 'daily_used': 0, 'monthly_limit': 3000, 'monthly_used': 0, 'max_attachment_size_kb': 5120}
2025-06-06 16:55:50 - app - INFO - [RAG] Request data: bot_id=None, conversation_id=4
2025-06-06 16:55:50 - app - INFO - [RAG] Documents found for conversation 4
2025-06-06 16:55:50 - app - INFO - [RAG] Using ONLY conversation documents for RAG
2025-06-06 16:55:50 - app - INFO - [RAG] Querying with: 'What can you tell me about this?...'
2025-06-06 16:55:50 - app - INFO - [RAG] Using ONLY conversation documents
2025-06-06 16:55:50 - app - INFO - [RAG] Started retrieval operation: rag_4_1749200150
2025-06-06 16:55:52 - app - INFO - [RAG] No relevant context found
2025-06-06 16:57:00 - app - INFO - Generating title for conversation 4 using model mistral-small3.1:latest
2025-06-06 16:57:00 - app - INFO - Calling LLM with 5 messages
2025-06-06 16:57:00 - app - INFO - Calling LLM with model mistral-small3.1:latest for title generation
2025-06-06 16:57:00 - app - INFO - Calling LLM model mistral-small3.1:latest with 5 messages (timeout: 5s)
2025-06-06 16:57:00 - app - INFO - Calling Ollama with model: mistral-small3.1:latest at host: http://localhost:11434
2025-06-06 16:57:17 - app - INFO - Logging configured successfully
2025-06-06 16:57:39 - app - INFO - Attempting to get available models...
2025-06-06 16:57:39 - app - INFO - User preferred models: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 16:57:39 - app - INFO - Available models from Ollama: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 16:57:39 - app - INFO - Filtered models based on user preferences: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 16:57:39 - app - INFO - Serialized models: [{'name': 'mistral-small3.1:latest', 'details': {'parameter_size': ''}}, {'name': 'mannix/llama3.1-8b-abliterated:latest', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}]
2025-06-06 16:57:52 - app - INFO - File 'aiss-technical-specs.pdf' saved successfully to 'C:\AIapps\MIDAS3\docs\conversation_5\aiss-technical-specs.pdf'
2025-06-06 16:57:59 - app - INFO - Quota check result: {'allowed': True, 'daily_limit': 100, 'daily_used': 0, 'monthly_limit': 3000, 'monthly_used': 0, 'max_attachment_size_kb': 5120}
2025-06-06 16:57:59 - app - INFO - [RAG] Request data: bot_id=None, conversation_id=5
2025-06-06 16:57:59 - app - INFO - [RAG] Documents found for conversation 5
2025-06-06 16:57:59 - app - INFO - [RAG] Using ONLY conversation documents for RAG
2025-06-06 16:57:59 - app - INFO - [RAG] Querying with: 'What can you tell me about this?...'
2025-06-06 16:57:59 - app - INFO - [RAG] Using ONLY conversation documents
2025-06-06 16:57:59 - app - INFO - [RAG] Started retrieval operation: rag_5_1749200279
2025-06-06 16:57:59 - app - INFO - [RAG] No relevant context found
2025-06-06 16:58:10 - app - INFO - Generating title for conversation 5 using model llama3.1:8b
2025-06-06 16:58:10 - app - INFO - Calling LLM with 5 messages
2025-06-06 16:58:10 - app - INFO - Calling LLM with model llama3.1:8b for title generation
2025-06-06 16:58:10 - app - INFO - Calling LLM model llama3.1:8b with 5 messages (timeout: 5s)
2025-06-06 16:58:10 - app - INFO - Calling Ollama with model: llama3.1:8b at host: http://localhost:11434
2025-06-06 16:58:11 - app - INFO - Raw LLM response: 'A Conversation Without Context'
2025-06-06 16:58:11 - app - INFO - Final generated title: A Conversation Without Context
2025-06-06 16:59:05 - app - INFO - Logging configured successfully
2025-06-06 17:00:02 - app - INFO - Attempting to get available models...
2025-06-06 17:00:02 - app - INFO - User preferred models: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:00:02 - app - INFO - Available models from Ollama: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:00:02 - app - INFO - Filtered models based on user preferences: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:00:02 - app - INFO - Serialized models: [{'name': 'mistral-small3.1:latest', 'details': {'parameter_size': ''}}, {'name': 'mannix/llama3.1-8b-abliterated:latest', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}]
2025-06-06 17:00:12 - app - INFO - File 'aiss-technical-specs.pdf' saved successfully to 'C:\AIapps\MIDAS3\docs\conversation_6\aiss-technical-specs.pdf'
2025-06-06 17:00:34 - app - INFO - Quota check result: {'allowed': True, 'daily_limit': 100, 'daily_used': 0, 'monthly_limit': 3000, 'monthly_used': 0, 'max_attachment_size_kb': 5120}
2025-06-06 17:00:34 - app - INFO - [RAG] Request data: bot_id=None, conversation_id=6
2025-06-06 17:00:34 - app - INFO - [RAG] Documents found for conversation 6
2025-06-06 17:00:34 - app - INFO - [RAG] Using ONLY conversation documents for RAG
2025-06-06 17:00:34 - app - INFO - [RAG] Querying with: 'What can you tell me about this?...'
2025-06-06 17:00:34 - app - INFO - [RAG] Using ONLY conversation documents
2025-06-06 17:00:34 - app - INFO - [RAG] Started retrieval operation: rag_6_1749200434
2025-06-06 17:00:34 - app - INFO - [RAG] No relevant context found
2025-06-06 17:00:38 - app - INFO - Generating title for conversation 6 using model llama3.1:8b
2025-06-06 17:00:38 - app - INFO - Calling LLM with 5 messages
2025-06-06 17:00:38 - app - INFO - Calling LLM with model llama3.1:8b for title generation
2025-06-06 17:00:38 - app - INFO - Calling LLM model llama3.1:8b with 5 messages (timeout: 5s)
2025-06-06 17:00:38 - app - INFO - Calling Ollama with model: llama3.1:8b at host: http://localhost:11434
2025-06-06 17:00:40 - app - INFO - Raw LLM response: 'Circular Question Conundrum'
2025-06-06 17:00:40 - app - INFO - Final generated title: Circular Question Conundrum
2025-06-06 17:01:41 - app - INFO - Logging configured successfully
2025-06-06 17:03:08 - app - INFO - File 'aiss-technical-specs.pdf' saved successfully to 'C:\AIapps\MIDAS3\docs\conversation_7\aiss-technical-specs.pdf'
2025-06-06 17:03:16 - app - INFO - Quota check result: {'allowed': True, 'daily_limit': 100, 'daily_used': 0, 'monthly_limit': 3000, 'monthly_used': 0, 'max_attachment_size_kb': 5120}
2025-06-06 17:03:16 - app - INFO - [RAG] Request data: bot_id=None, conversation_id=7
2025-06-06 17:03:16 - app - INFO - [RAG] Documents found for conversation 7
2025-06-06 17:03:16 - app - INFO - [RAG] Using ONLY conversation documents for RAG
2025-06-06 17:03:16 - app - INFO - [RAG] Querying with: 'What can you tell me about this?...'
2025-06-06 17:03:16 - app - INFO - [RAG] Using ONLY conversation documents
2025-06-06 17:03:16 - app - INFO - [RAG] Started retrieval operation: rag_7_1749200596
2025-06-06 17:03:16 - app - INFO - [RAG] Retrieved 4431 characters in 0.06s
2025-06-06 17:03:16 - app - INFO - [RAG] Context sample: '

Retrieved 61 relevant context chunks (9.3% of knowledge base) for query: 'What can you tell me about this?'

--- Context Chunk 1 (Relevance: 0.70) ---
Source: Unknown source

A


--- Context Chunk 2...'
2025-06-06 17:03:16 - app - INFO - [RAG] Enhanced system prompt with 4431 characters of knowledge
2025-06-06 17:03:20 - app - INFO - Generating title for conversation 7 using model llama3.1:8b
2025-06-06 17:03:20 - app - INFO - Calling LLM with 5 messages
2025-06-06 17:03:20 - app - INFO - Calling LLM with model llama3.1:8b for title generation
2025-06-06 17:03:20 - app - INFO - Calling LLM model llama3.1:8b with 5 messages (timeout: 5s)
2025-06-06 17:03:20 - app - INFO - Calling Ollama with model: llama3.1:8b at host: http://localhost:11434
2025-06-06 17:03:21 - app - INFO - Raw LLM response: 'Circular Conversation Conundrum'
2025-06-06 17:03:21 - app - INFO - Final generated title: Circular Conversation Conundrum
2025-06-06 17:04:04 - app - INFO - Logging configured successfully
2025-06-06 17:04:07 - app - INFO - Logging configured successfully
2025-06-06 17:05:44 - app - INFO - Logging configured successfully
2025-06-06 17:05:56 - app - INFO - Logging configured successfully
2025-06-06 17:06:42 - app - INFO - Attempting to get available models...
2025-06-06 17:06:42 - app - INFO - User preferred models: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:06:42 - app - INFO - Available models from Ollama: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:06:42 - app - INFO - Filtered models based on user preferences: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:06:42 - app - INFO - Serialized models: [{'name': 'mistral-small3.1:latest', 'details': {'parameter_size': ''}}, {'name': 'mannix/llama3.1-8b-abliterated:latest', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}]
2025-06-06 17:06:53 - app - INFO - File 'aiss-technical-specs.pdf' saved successfully to 'C:\AIapps\MIDAS3\docs\conversation_8\aiss-technical-specs.pdf'
2025-06-06 17:06:54 - app - ERROR - Failed to index document: aiss-technical-specs.pdf
2025-06-06 17:07:38 - app - INFO - Logging configured successfully
2025-06-06 17:07:49 - app - INFO - Logging configured successfully
2025-06-06 17:08:24 - app - INFO - Attempting to get available models...
2025-06-06 17:08:24 - app - INFO - User preferred models: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:08:24 - app - INFO - Available models from Ollama: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:08:24 - app - INFO - Filtered models based on user preferences: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:08:24 - app - INFO - Serialized models: [{'name': 'mistral-small3.1:latest', 'details': {'parameter_size': ''}}, {'name': 'mannix/llama3.1-8b-abliterated:latest', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}]
2025-06-06 17:08:34 - app - INFO - File 'aiss-technical-specs.pdf' saved successfully to 'C:\AIapps\MIDAS3\docs\conversation_8\aiss-technical-specs.pdf'
2025-06-06 17:08:52 - app - INFO - Quota check result: {'allowed': True, 'daily_limit': 100, 'daily_used': 0, 'monthly_limit': 3000, 'monthly_used': 0, 'max_attachment_size_kb': 5120}
2025-06-06 17:08:52 - app - INFO - [RAG] Request data: bot_id=None, conversation_id=8
2025-06-06 17:08:52 - app - INFO - [RAG] Documents found for conversation 8
2025-06-06 17:08:52 - app - INFO - [RAG] Using ONLY conversation documents for RAG
2025-06-06 17:08:52 - app - INFO - [RAG] Querying with: 'What can you tell me about this document?...'
2025-06-06 17:08:52 - app - INFO - [RAG] Using ONLY conversation documents
2025-06-06 17:08:52 - app - INFO - [RAG] Started retrieval operation: rag_8_1749200932
2025-06-06 17:08:52 - app - INFO - [RAG] Retrieved 3916 characters in 0.07s
2025-06-06 17:08:52 - app - INFO - [RAG] Context sample: '

Retrieved 59 relevant context chunks (8.9% of knowledge base) for query: 'What can you tell me about this document?'
Sources consulted: 1 documents

--- Context Chunk 1 (Relevance: 0.70) ---
Source:...'
2025-06-06 17:08:52 - app - INFO - [RAG] Enhanced system prompt with 3916 characters of knowledge
2025-06-06 17:10:11 - app - INFO - Logging configured successfully
2025-06-06 17:10:29 - app - INFO - Logging configured successfully
2025-06-06 17:10:38 - app - INFO - Logging configured successfully
2025-06-06 17:11:57 - app - INFO - Attempting to get available models...
2025-06-06 17:11:57 - app - INFO - User preferred models: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:11:57 - app - INFO - Available models from Ollama: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:11:57 - app - INFO - Filtered models based on user preferences: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:11:57 - app - INFO - Serialized models: [{'name': 'mistral-small3.1:latest', 'details': {'parameter_size': ''}}, {'name': 'mannix/llama3.1-8b-abliterated:latest', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}]
2025-06-06 17:13:23 - app - INFO - File 'aiss-technical-specs.pdf' saved successfully to 'C:\AIapps\MIDAS3\docs\conversation_9\aiss-technical-specs.pdf'
2025-06-06 17:13:39 - app - INFO - Quota check result: {'allowed': True, 'daily_limit': 100, 'daily_used': 0, 'monthly_limit': 3000, 'monthly_used': 0, 'max_attachment_size_kb': 5120}
2025-06-06 17:13:39 - app - INFO - [RAG] Request data: bot_id=None, conversation_id=9
2025-06-06 17:13:39 - app - INFO - [RAG] Documents found for conversation 9
2025-06-06 17:13:39 - app - INFO - [RAG] Using ONLY conversation documents for RAG
2025-06-06 17:13:39 - app - INFO - [RAG] Querying with: 'What can you tell me about this?...'
2025-06-06 17:13:39 - app - INFO - [RAG] Using ONLY conversation documents
2025-06-06 17:13:39 - app - INFO - [RAG] Started retrieval operation: rag_9_1749201219
2025-06-06 17:13:39 - app - INFO - [RAG] Retrieved 3907 characters in 0.07s
2025-06-06 17:13:39 - app - INFO - [RAG] Context sample: '

Retrieved 59 relevant context chunks (8.7% of knowledge base) for query: 'What can you tell me about this?'
Sources consulted: 1 documents

--- Context Chunk 1 (Relevance: 0.70) ---
Source: unknown
...'
2025-06-06 17:13:39 - app - INFO - [RAG] Enhanced system prompt with 3907 characters of knowledge
2025-06-06 17:13:47 - app - INFO - Generating title for conversation 9 using model llama3.1:8b
2025-06-06 17:13:47 - app - INFO - Calling LLM with 5 messages
2025-06-06 17:13:47 - app - INFO - Calling LLM with model llama3.1:8b for title generation
2025-06-06 17:13:47 - app - INFO - Calling LLM model llama3.1:8b with 5 messages (timeout: 5s)
2025-06-06 17:13:47 - app - INFO - Calling Ollama with model: llama3.1:8b at host: http://localhost:11434
2025-06-06 17:13:48 - app - INFO - Raw LLM response: 'Circular Question Response'
2025-06-06 17:13:48 - app - INFO - Final generated title: Circular Question Response
2025-06-06 17:15:21 - app - INFO - Logging configured successfully
2025-06-06 17:15:33 - app - INFO - Logging configured successfully
2025-06-06 17:16:36 - app - INFO - Attempting to get available models...
2025-06-06 17:16:36 - app - INFO - User preferred models: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:16:36 - app - INFO - Available models from Ollama: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:16:36 - app - INFO - Filtered models based on user preferences: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:16:36 - app - INFO - Serialized models: [{'name': 'mistral-small3.1:latest', 'details': {'parameter_size': ''}}, {'name': 'mannix/llama3.1-8b-abliterated:latest', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}]
2025-06-06 17:16:48 - app - INFO - File 'aiss-technical-specs.pdf' saved successfully to 'C:\AIapps\MIDAS3\docs\conversation_10\aiss-technical-specs.pdf'
2025-06-06 17:16:58 - app - INFO - Quota check result: {'allowed': True, 'daily_limit': 100, 'daily_used': 0, 'monthly_limit': 3000, 'monthly_used': 0, 'max_attachment_size_kb': 5120}
2025-06-06 17:16:58 - app - INFO - [RAG] Request data: bot_id=None, conversation_id=10
2025-06-06 17:16:58 - app - INFO - [RAG] Documents found for conversation 10
2025-06-06 17:16:58 - app - INFO - [RAG] Using ONLY conversation documents for RAG
2025-06-06 17:16:58 - app - INFO - [RAG] Querying with: 'What can you tell me about this?...'
2025-06-06 17:16:58 - app - INFO - [RAG] Using ONLY conversation documents
2025-06-06 17:16:58 - app - INFO - [RAG] Started retrieval operation: rag_10_1749201418
2025-06-06 17:16:58 - app - INFO - [RAG] Retrieved 3907 characters in 0.07s
2025-06-06 17:16:58 - app - INFO - [RAG] Context sample: '

Retrieved 59 relevant context chunks (8.7% of knowledge base) for query: 'What can you tell me about this?'
Sources consulted: 1 documents

--- Context Chunk 1 (Relevance: 0.70) ---
Source: unknown
...'
2025-06-06 17:16:58 - app - INFO - [RAG] Enhanced system prompt with 3907 characters of knowledge
2025-06-06 17:17:02 - app - INFO - Generating title for conversation 10 using model llama3.1:8b
2025-06-06 17:17:02 - app - INFO - Calling LLM with 5 messages
2025-06-06 17:17:02 - app - INFO - Calling LLM with model llama3.1:8b for title generation
2025-06-06 17:17:02 - app - INFO - Calling LLM model llama3.1:8b with 5 messages (timeout: 5s)
2025-06-06 17:17:02 - app - INFO - Calling Ollama with model: llama3.1:8b at host: http://localhost:11434
2025-06-06 17:17:03 - app - INFO - Raw LLM response: 'Contextual Information Missing'
2025-06-06 17:17:03 - app - INFO - Final generated title: Contextual Information Missing
2025-06-06 17:18:11 - app - INFO - Logging configured successfully
2025-06-06 17:18:34 - app - INFO - Logging configured successfully
2025-06-06 17:19:12 - app - INFO - Attempting to get available models...
2025-06-06 17:19:12 - app - INFO - User preferred models: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:19:12 - app - INFO - Available models from Ollama: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:19:12 - app - INFO - Filtered models based on user preferences: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:19:12 - app - INFO - Serialized models: [{'name': 'mistral-small3.1:latest', 'details': {'parameter_size': ''}}, {'name': 'mannix/llama3.1-8b-abliterated:latest', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}]
2025-06-06 17:19:31 - app - INFO - File 'aiss-technical-specs.pdf' saved successfully to 'C:\AIapps\MIDAS3\docs\conversation_11\aiss-technical-specs.pdf'
2025-06-06 17:19:31 - app - ERROR - Failed to index document: aiss-technical-specs.pdf
2025-06-06 17:20:20 - app - INFO - Logging configured successfully
2025-06-06 17:20:48 - app - INFO - Logging configured successfully
2025-06-06 17:21:20 - app - INFO - Attempting to get available models...
2025-06-06 17:21:20 - app - INFO - User preferred models: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:21:20 - app - INFO - Available models from Ollama: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:21:20 - app - INFO - Filtered models based on user preferences: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:21:20 - app - INFO - Serialized models: [{'name': 'mistral-small3.1:latest', 'details': {'parameter_size': ''}}, {'name': 'mannix/llama3.1-8b-abliterated:latest', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}]
2025-06-06 17:21:42 - app - INFO - File 'aiss-technical-specs.pdf' saved successfully to 'C:\AIapps\MIDAS3\docs\conversation_11\aiss-technical-specs.pdf'
2025-06-06 17:22:02 - app - INFO - Quota check result: {'allowed': True, 'daily_limit': 100, 'daily_used': 0, 'monthly_limit': 3000, 'monthly_used': 0, 'max_attachment_size_kb': 5120}
2025-06-06 17:22:02 - app - INFO - [RAG] Request data: bot_id=None, conversation_id=11
2025-06-06 17:22:02 - app - INFO - [RAG] Documents found for conversation 11
2025-06-06 17:22:02 - app - INFO - [RAG] Using ONLY conversation documents for RAG
2025-06-06 17:22:02 - app - INFO - [RAG] Querying with: 'What can you tell me about this document?...'
2025-06-06 17:22:02 - app - INFO - [RAG] Using ONLY conversation documents
2025-06-06 17:22:02 - app - INFO - [RAG] Started retrieval operation: rag_11_1749201722
2025-06-06 17:22:03 - app - INFO - [RAG] Retrieved 3916 characters in 0.12s
2025-06-06 17:22:03 - app - INFO - [RAG] Context sample: '

Retrieved 59 relevant context chunks (8.7% of knowledge base) for query: 'What can you tell me about this document?'
Sources consulted: 1 documents

--- Context Chunk 1 (Relevance: 0.70) ---
Source:...'
2025-06-06 17:22:03 - app - INFO - [RAG] Enhanced system prompt with 3916 characters of knowledge
2025-06-06 17:22:08 - app - INFO - Generating title for conversation 11 using model llama3.1:8b
2025-06-06 17:22:08 - app - INFO - Calling LLM with 5 messages
2025-06-06 17:22:08 - app - INFO - Calling LLM with model llama3.1:8b for title generation
2025-06-06 17:22:08 - app - INFO - Calling LLM model llama3.1:8b with 5 messages (timeout: 5s)
2025-06-06 17:22:08 - app - INFO - Calling Ollama with model: llama3.1:8b at host: http://localhost:11434
2025-06-06 17:22:09 - app - INFO - Raw LLM response: 'Discussion About an Unspecified Document'
2025-06-06 17:22:09 - app - INFO - Final generated title: Discussion About An Unspecified Document
2025-06-06 17:25:07 - app - INFO - Logging configured successfully
2025-06-06 17:26:11 - app - INFO - Logging configured successfully
2025-06-06 17:26:25 - app - INFO - Attempting to get available models...
2025-06-06 17:26:25 - app - INFO - User preferred models: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:26:25 - app - INFO - Available models from Ollama: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:26:25 - app - INFO - Filtered models based on user preferences: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:26:25 - app - INFO - Serialized models: [{'name': 'mistral-small3.1:latest', 'details': {'parameter_size': ''}}, {'name': 'mannix/llama3.1-8b-abliterated:latest', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}]
2025-06-06 17:26:38 - app - INFO - File 'aiss-technical-specs.pdf' saved successfully to 'C:\AIapps\MIDAS3\docs\conversation_13\aiss-technical-specs.pdf'
2025-06-06 17:26:47 - app - INFO - Quota check result: {'allowed': True, 'daily_limit': 100, 'daily_used': 0, 'monthly_limit': 3000, 'monthly_used': 0, 'max_attachment_size_kb': 5120}
2025-06-06 17:26:47 - app - INFO - [RAG] Request data: bot_id=None, conversation_id=13
2025-06-06 17:26:47 - app - INFO - [RAG] Documents found for conversation 13
2025-06-06 17:26:47 - app - INFO - [RAG] Using ONLY conversation documents for RAG
2025-06-06 17:26:47 - app - INFO - [RAG] Querying with: 'What can you tell me about this?...'
2025-06-06 17:26:47 - app - INFO - [RAG] Using ONLY conversation documents
2025-06-06 17:26:47 - app - INFO - [RAG] Started retrieval operation: rag_13_1749202007
2025-06-06 17:26:47 - app - INFO - [RAG] Retrieved 3907 characters in 0.07s
2025-06-06 17:26:47 - app - INFO - [RAG] Context sample: '

Retrieved 59 relevant context chunks (8.7% of knowledge base) for query: 'What can you tell me about this?'
Sources consulted: 1 documents

--- Context Chunk 1 (Relevance: 0.70) ---
Source: unknown
...'
2025-06-06 17:26:47 - app - INFO - [RAG] Enhanced system prompt with 3907 characters of knowledge
2025-06-06 17:26:51 - app - INFO - Generating title for conversation 13 using model llama3.1:8b
2025-06-06 17:26:51 - app - INFO - Calling LLM with 5 messages
2025-06-06 17:26:51 - app - INFO - Calling LLM with model llama3.1:8b for title generation
2025-06-06 17:26:51 - app - INFO - Calling LLM model llama3.1:8b with 5 messages (timeout: 5s)
2025-06-06 17:26:51 - app - INFO - Calling Ollama with model: llama3.1:8b at host: http://localhost:11434
2025-06-06 17:26:52 - app - INFO - Raw LLM response: 'Circular Conversation with No Context'
2025-06-06 17:26:52 - app - INFO - Final generated title: Circular Conversation With No Context
2025-06-06 17:36:11 - app - INFO - Logging configured successfully
2025-06-06 17:36:15 - app - INFO - Logging configured successfully
2025-06-06 17:36:19 - app - INFO - Attempting to get available models...
2025-06-06 17:36:19 - app - INFO - User preferred models: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:36:19 - app - INFO - Available models from Ollama: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:36:19 - app - INFO - Filtered models based on user preferences: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:36:19 - app - INFO - Serialized models: [{'name': 'mistral-small3.1:latest', 'details': {'parameter_size': ''}}, {'name': 'mannix/llama3.1-8b-abliterated:latest', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}]
2025-06-06 17:36:34 - app - INFO - File 'aiss-technical-specs.pdf' saved successfully to 'C:\AIapps\MIDAS3\docs\conversation_14\aiss-technical-specs.pdf'
2025-06-06 17:36:48 - app - INFO - Quota check result: {'allowed': True, 'daily_limit': 100, 'daily_used': 0, 'monthly_limit': 3000, 'monthly_used': 0, 'max_attachment_size_kb': 5120}
2025-06-06 17:36:48 - app - INFO - [RAG] Request data: bot_id=None, conversation_id=14
2025-06-06 17:36:48 - app - INFO - [RAG] Documents found for conversation 14
2025-06-06 17:36:48 - app - INFO - [RAG] Using ONLY conversation documents for RAG
2025-06-06 17:36:48 - app - INFO - [RAG] Querying with: 'What can you tell me about this?...'
2025-06-06 17:36:48 - app - INFO - [RAG] Using ONLY conversation documents
2025-06-06 17:36:48 - app - INFO - [RAG] Started retrieval operation: rag_14_1749202608
2025-06-06 17:36:48 - app - INFO - [RAG] Retrieved 4431 characters in 0.12s
2025-06-06 17:36:48 - app - INFO - [RAG] Context sample: '

Retrieved 61 relevant context chunks (9.3% of knowledge base) for query: 'What can you tell me about this?'

--- Context Chunk 1 (Relevance: 0.70) ---
Source: Unknown source

A


--- Context Chunk 2...'
2025-06-06 17:36:48 - app - INFO - [RAG] Enhanced system prompt with 4431 characters of knowledge
2025-06-06 17:36:54 - app - INFO - Generating title for conversation 14 using model llama3.1:8b
2025-06-06 17:36:54 - app - INFO - Calling LLM with 5 messages
2025-06-06 17:36:54 - app - INFO - Calling LLM with model llama3.1:8b for title generation
2025-06-06 17:36:54 - app - INFO - Calling LLM model llama3.1:8b with 5 messages (timeout: 5s)
2025-06-06 17:36:54 - app - INFO - Calling Ollama with model: llama3.1:8b at host: http://localhost:11434
2025-06-06 17:36:55 - app - INFO - Raw LLM response: 'Circular Conversation of No Context'
2025-06-06 17:36:55 - app - INFO - Final generated title: Circular Conversation Of No Context
2025-06-06 17:37:57 - app - INFO - Logging configured successfully
2025-06-06 17:38:13 - app - INFO - Attempting to get available models...
2025-06-06 17:38:13 - app - INFO - User preferred models: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:38:13 - app - INFO - Available models from Ollama: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:38:13 - app - INFO - Filtered models based on user preferences: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:38:13 - app - INFO - Serialized models: [{'name': 'mistral-small3.1:latest', 'details': {'parameter_size': ''}}, {'name': 'mannix/llama3.1-8b-abliterated:latest', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}]
2025-06-06 17:38:26 - app - INFO - File 'aiss-technical-specs.pdf' saved successfully to 'C:\AIapps\MIDAS3\docs\conversation_15\aiss-technical-specs.pdf'
2025-06-06 17:38:34 - app - INFO - Quota check result: {'allowed': True, 'daily_limit': 100, 'daily_used': 0, 'monthly_limit': 3000, 'monthly_used': 0, 'max_attachment_size_kb': 5120}
2025-06-06 17:38:34 - app - INFO - [RAG] Request data: bot_id=None, conversation_id=15
2025-06-06 17:38:34 - app - INFO - [RAG] Documents found for conversation 15
2025-06-06 17:38:34 - app - INFO - [RAG] Using ONLY conversation documents for RAG
2025-06-06 17:38:34 - app - INFO - [RAG] Querying with: 'What can you tell me about this?...'
2025-06-06 17:38:34 - app - INFO - [RAG] Using ONLY conversation documents
2025-06-06 17:38:34 - app - INFO - [RAG] Started retrieval operation: rag_15_1749202714
2025-06-06 17:38:35 - app - INFO - [RAG] Retrieved 4644 characters in 0.12s
2025-06-06 17:38:35 - app - INFO - [RAG] Context sample: '

Retrieved 64 relevant context chunks (8.5% of knowledge base) for query: 'What can you tell me about this?'

--- Context Chunk 1 (Relevance: 0.70) ---
Source: Unknown source

-


--- Context Chunk 2...'
2025-06-06 17:38:35 - app - INFO - [RAG] Enhanced system prompt with 4644 characters of knowledge
2025-06-06 17:38:42 - app - INFO - Generating title for conversation 15 using model llama3.1:8b
2025-06-06 17:38:42 - app - INFO - Calling LLM with 5 messages
2025-06-06 17:38:42 - app - INFO - Calling LLM with model llama3.1:8b for title generation
2025-06-06 17:38:42 - app - INFO - Calling LLM model llama3.1:8b with 5 messages (timeout: 5s)
2025-06-06 17:38:42 - app - INFO - Calling Ollama with model: llama3.1:8b at host: http://localhost:11434
2025-06-06 17:38:43 - app - INFO - Raw LLM response: 'Circular Conversation Loop Detected'
2025-06-06 17:38:43 - app - INFO - Final generated title: Circular Conversation Loop Detected
2025-06-06 17:39:56 - app - INFO - Logging configured successfully
2025-06-06 17:40:19 - app - INFO - Attempting to get available models...
2025-06-06 17:40:19 - app - INFO - User preferred models: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:40:19 - app - INFO - Available models from Ollama: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:40:19 - app - INFO - Filtered models based on user preferences: ['mistral-small3.1:latest', 'mannix/llama3.1-8b-abliterated:latest', 'deepseek-r1:7b', 'phi3.5:latest', 'mistral:latest', 'llama3.1:8b', 'codellama:latest']
2025-06-06 17:40:19 - app - INFO - Serialized models: [{'name': 'mistral-small3.1:latest', 'details': {'parameter_size': ''}}, {'name': 'mannix/llama3.1-8b-abliterated:latest', 'details': {'parameter_size': '7B-8B'}}, {'name': 'deepseek-r1:7b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'phi3.5:latest', 'details': {'parameter_size': ''}}, {'name': 'mistral:latest', 'details': {'parameter_size': ''}}, {'name': 'llama3.1:8b', 'details': {'parameter_size': '7B-8B'}}, {'name': 'codellama:latest', 'details': {'parameter_size': ''}}]
2025-06-06 17:40:34 - app - INFO - File 'aiss-technical-specs.pdf' saved successfully to 'C:\AIapps\MIDAS3\docs\conversation_16\aiss-technical-specs.pdf'
2025-06-06 17:41:03 - app - INFO - Quota check result: {'allowed': True, 'daily_limit': 100, 'daily_used': 0, 'monthly_limit': 3000, 'monthly_used': 0, 'max_attachment_size_kb': 5120}
2025-06-06 17:41:03 - app - INFO - [RAG] Request data: bot_id=None, conversation_id=16
2025-06-06 17:41:03 - app - INFO - [RAG] Documents found for conversation 16
2025-06-06 17:41:03 - app - INFO - [RAG] Using ONLY conversation documents for RAG
2025-06-06 17:41:03 - app - INFO - [RAG] Querying with: 'What can you tell me about this?...'
2025-06-06 17:41:03 - app - INFO - [RAG] Using ONLY conversation documents
2025-06-06 17:41:03 - app - INFO - [RAG] Started retrieval operation: rag_16_1749202863
2025-06-06 17:41:04 - app - INFO - [RAG] Retrieved 4644 characters in 0.11s
2025-06-06 17:41:04 - app - INFO - [RAG] Context sample: '

Retrieved 64 relevant context chunks (8.5% of knowledge base) for query: 'What can you tell me about this?'

--- Context Chunk 1 (Relevance: 0.70) ---
Source: Unknown source

-


--- Context Chunk 2...'
2025-06-06 17:41:04 - app - INFO - [RAG] Enhanced system prompt with 4644 characters of knowledge
2025-06-06 17:41:08 - app - INFO - Generating title for conversation 16 using model llama3.1:8b
2025-06-06 17:41:08 - app - INFO - Calling LLM with 5 messages
2025-06-06 17:41:08 - app - INFO - Calling LLM with model llama3.1:8b for title generation
2025-06-06 17:41:08 - app - INFO - Calling LLM model llama3.1:8b with 5 messages (timeout: 5s)
2025-06-06 17:41:08 - app - INFO - Calling Ollama with model: llama3.1:8b at host: http://localhost:11434
2025-06-06 17:41:09 - app - INFO - Raw LLM response: 'Lack of Contextual Information Provided'
2025-06-06 17:41:09 - app - INFO - Final generated title: Lack Of Contextual Information Provided
